import os
import gym
import json
import logging
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import keras
import tensorflow as tf
from gym import spaces
from gym.utils import seeding
from dataprep.dataset import load_reformated_csv, create_dataset
from globals import *

# ----------- This part is to avoid any CUBLAS errors ------------------------------
print( tf.config.list_physical_devices( 'GPU' ) )  # check for the GPU, if being used
gpus = tf.config.experimental.list_physical_devices ( 'GPU' )
if gpus :
    try :
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus :
            tf.config.experimental.set_memory_growth ( gpu , True )
        logical_gpus = tf.config.experimental.list_logical_devices ( 'GPU' )
        print ( len ( gpus ) , "Physical GPUs," , len ( logical_gpus ) , "Logical GPUs" )
    except RuntimeError as e :
        # Memory growth must be set before GPUs have been initialized
        print ( e )
# ----------- This part is to avoid any CUBLAS errors ------------------------------

logging.basicConfig ( format = '%(asctime)s - %(levelname)s - %(message)s' )
logger = logging.getLogger ( 'RL-Logger' )
logger.setLevel ( logging.INFO )

np.seterr ( divide = 'ignore' , invalid = 'ignore' )

def get_dataset ( df ,
                  variable: str = 'B:VIMIN',
                  train_test_split: float = 0.7) :
    dataset = df [ variable ].values
    dataset = dataset.astype ( 'float32' )
    dataset = np.reshape ( dataset , (-1 , 1) )

    train_size = int ( len ( dataset ) * train_test_split )
    train , test = dataset [ 0 :train_size , : ] , dataset [ train_size :len ( dataset ) , : ]

    X_train , Y_train = create_dataset ( train ,
                                         look_back = LOOK_BACK,
                                         look_forward = LOOK_FORWARD)
    X_train = np.reshape ( X_train , (X_train.shape [ 0 ] , X_train.shape [ 1 ], 1) )
    Y_train = np.reshape ( Y_train , (Y_train.shape [ 0 ] , Y_train.shape [ 1 ]) )

    return X_train , Y_train

def all_inplace_scale ( df ) :
    scale_dict = {}

    for var in VARIABLES :
        our_data2 = df
        trace = our_data2 [ var ].astype ( 'float32' )
        data = np.array ( trace )

        median = np.median ( data )
        upper_quartile = np.percentile ( data , 75 )
        lower_quartile = np.percentile ( data , 25 )

        iqr = upper_quartile - lower_quartile
        lower_whisker = data [ data >= lower_quartile - 1.5 * iqr ].min ( )
        upper_whisker = data [ data <= upper_quartile + 1.5 * iqr ].max ( )

        ranged = upper_whisker - lower_whisker
        # (value âˆ’ median) / (upper - lower)
        our_data2 [ var ] = 1 / ranged * (data - median)

        scale_dict [ str ( var ) ] = {"median" : median , "range" : ranged}

    return scale_dict


def unscale ( var_name , tseries , scale_dict ) :
    # equivalent to inverse transform
    from_model = np.asarray ( tseries )
    update = from_model * scale_dict [ str ( var_name ) ] [ "range" ] + scale_dict [ str ( var_name ) ] [ "median" ]

    return (update)


def rescale ( var_name , tseries , scale_dict ) :
    # equivalent to transform
    data = np.asarray ( tseries )
    update = 1 / scale_dict [ str ( var_name ) ] [ "range" ] * (data - scale_dict [ str ( var_name ) ] [ "median" ])

    return (update)

def create_dropout_predict_model ( model , dropout ) :
    conf = model.get_config ( )

    for layer in conf [ 'layers' ] :
        # Dropout layers
        if layer [ "class_name" ] == "Dropout" :
            # print(layer)
            layer [ "config" ] [ "rate" ] = dropout
    model_dropout = tf.keras.Model.from_config ( conf )
    model_dropout.set_weights ( model.get_weights ( ) )
    return model_dropout

class ExaBooster_v1(gym.Env):

    def __init__(self):
        self.episodes = 0
        self.steps = 0
        self.total_reward = 0
        self.data_total_reward = 0
        self.total_iminer = 0
        self.data_total_iminer = 0
        self.diff = 0
        self.max_steps = 100
        self.nactions = 25

        # Define boundary
        self.min_BIMIN = 103.1
        self.max_BIMIN = 103.6

        self.variables = VARIABLES
        self.nvariables = len(self.variables)
        logger.info('Number of variables:{}'.format(self.nvariables))

        # Load surrogate models
        model = keras.models.load_model (
            LATEST_SURROGATE_MODEL ,
            compile = False )

        self.booster_model = create_dropout_predict_model ( model , 0 )

        # Load data to initialize the env
        with open(DATA_CONFIG) as json_file:
            data_config = json.load(json_file)
        data = load_reformated_csv(filename = data_config['data_dir'] + data_config['data_filename'],
                                   nrows = NSTEPS)
        data['B:VIMIN'] = data['B:VIMIN'].shift(-1)
        data = data.set_index(pd.to_datetime(data.time))
        data = data.dropna()
        data = data.drop_duplicates()
        logger.info ( 'Number of samples:{}'.format ( data.shape ) )
        self.scalers = all_inplace_scale ( data )
        data_list = []
        x_train = []
        for v in range(len(self.variables)):
            data_list.append(get_dataset(data, variable=self.variables[v]))
            x_train.append(data_list[v][0])
        # Axis
        concate_axis = 2
        self.X_train = np.concatenate(x_train, axis=concate_axis)
        print( 'Data shape:{}'.format( self.X_train.shape ) )
        self.nbatches = self.X_train.shape[0]
        self.nsamples = self.X_train.shape[1]

        if IN_PLAY_MODE:
            self.batch_id = self.episodes + 2500  # to test we need samples that are not random
        else:
            self.batch_id = np.random.randint( low = 500 , high = 5000 )

        self.data_state = None
        self.observation_space = spaces.Box(
            low=0,
            high=+1,
            shape=(self.nvariables,),
            dtype=np.float64
        )

        # Dynamically allocate
        data['B:VIMIN_DIFF'] = data['B:VIMIN'] - data['B:VIMIN'].shift(-1)
        data = data.dropna()
        data = data.drop_duplicates()
        self.action_space = spaces.Discrete(self.nactions)
        self.actionMap_VIMIN = []
        for i in range(1, self.nactions + 1):
            self.actionMap_VIMIN.append(data['B:VIMIN_DIFF'].quantile(i / (self.nactions + 1)))

        self.VIMIN = 0
        # self.state = np.zeros(shape=(1, self.nvariables, self.nsamples))
        self.state = np.zeros(shape=(1, self.nsamples, self.nvariables))
        self.predicted_state = np.zeros(shape=(1, self.nvariables, 1))
        logger.debug('Init pred shape:{}'.format(self.predicted_state.shape))
        self.do_render = True  # True

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        self.steps += 1
        done = False

        # Steps:
        # 1) Update VIMIN based on action
        # 2) Predict booster dynamic variables
        # 3) Shift state with new values
        # 4) Update B:IMINER

        # Step 1: Calculate the new B:VINMIN based on policy action
        logger.info('Step() before action VIMIN:{}'.format(self.VIMIN))
        delta_VIMIN = self.actionMap_VIMIN[action]
        # DENORN_BVIMIN = self.scalers[0].inverse_transform(np.array([self.VIMIN]).reshape(1, -1))
        DENORN_BVIMIN = unscale ( self.variables [ 0 ] , np.array ( [ self.VIMIN ] ).reshape ( 1 , -1 ) ,
                        self.scalers )
        DENORN_BVIMIN += delta_VIMIN
        logger.debug('Step() descaled VIMIN:{}'.format(DENORN_BVIMIN))

        if DENORN_BVIMIN < self.min_BIMIN or DENORN_BVIMIN > self.max_BIMIN:
            logger.info('Step() descaled VIMIN:{} is out of bounds.'.format(DENORN_BVIMIN))
            done = True

        # self.VIMIN = self.scalers[0].transform(DENORN_BVIMIN)
        self.VIMIN = rescale ( self.variables [ 0 ] , DENORN_BVIMIN ,
                               self.scalers )
        logger.debug('Step() updated VIMIN:{}'.format(self.VIMIN))
        # self.state[0][0][self.nsamples - 1] = self.VIMIN
        self.state[0][self.nsamples - 1][0] = self.VIMIN

        # Step 2: Predict using booster model
        self.predicted_state = self.booster_model.predict(self.state)
        self.predicted_state = self.predicted_state.reshape(1, OUTPUTS, 1)

        # Step 3: Shift state by one step
        # self.state [ 0 , : , 0 :-1 ] = self.state [ 0 , : , 1 : ]  # shift forward
        self.state [ 0 , 0 :-1,: ] = self.state [ 0 , 1 :, : ]
        # Step 4: Update IMINER
        # self.state [ 0 ] [ 1 ] [ self.nsamples - 1 ] = self.predicted_state [ 0 , 1 :2 ]
        self.state [0][self.nsamples - 1] [1] = self.predicted_state[0, 1:2]

        # Update data state for rendering
        self.data_state = np.copy(self.X_train[self.batch_id + self.steps].reshape(1, self.nsamples, self.nvariables))
        # data_iminer = self.scalers[1].inverse_transform(self.data_state[0][1][self.nsamples - 1].reshape(1, -1))
        data_iminer =  unscale ( self.variables [ 1 ] ,
                                self.data_state [ 0 ] [ self.nsamples - 1 ] [ 1 ].reshape ( 1 , -1 ) ,
                                self.scalers )
        data_reward = -abs(data_iminer)

        # Use data for everything but the B:IMINER prediction
        # self.state[0, 2:self.nvariables, :] = self.data_state[0, 2:self.nvariables, :]
        self.state[0 , :, 2 :self.nvariables] = self.data_state[0 , :, 2 :self.nvariables]
        iminer = self.predicted_state[0, 1]
        logger.debug('norm iminer:{}'.format(iminer))
        # iminer = self.scalers[1].inverse_transform(np.array([iminer]).reshape(1, -1))
        iminer = unscale ( self.variables [ 1 ] , np.array ( [ iminer ] ) , self.scalers ).reshape ( 1 ,
                                                                                                        -1 )
        logger.debug('iminer:{}'.format(iminer))

        # Reward
        reward = -abs(iminer)

        if abs(iminer) >= 2:
            logger.info('iminer:{} is out of bounds'.format(iminer))
            done = True
            penalty = 5 * (self.max_steps - self.steps)
            logger.info ( 'penalty:{} is out of bounds'.format ( penalty ) )
            reward -= penalty

        self.diff += np.asscalar(abs(data_iminer - iminer))
        self.data_total_reward += np.asscalar(data_reward)
        self.total_reward += np.asscalar(reward)
        self.total_iminer  += np.asscalar(abs(iminer))
        self.data_total_iminer += np.asscalar(abs(data_iminer))

        if self.steps >= int(self.max_steps):
            done = True

        if done == True:
            DATA_REWARD_LIST.append( self.data_total_reward)
            RL_REWARD_LIST.append( self.total_reward)
            IMINER_IMP_LIST.append( self.data_total_iminer / self.total_iminer )

        if self.do_render:
            self.render()

        # return self.state[0, :, -1:].flatten(), np.asscalar(reward), done, {}
        return self.state [0, -1:, :].flatten(), np.asscalar(reward), done, {}

    def reset(self):
        self.episodes += 1
        self.steps = 0
        self.data_total_reward = 0
        self.total_reward = 0
        self.total_iminer = 0
        self.data_total_iminer = 0
        self.diff = 0
        self.data_state = None

        if IN_PLAY_MODE:
            self.batch_id = self.episodes + 2500  # to test we need samples that are not random
        else:
            self.batch_id = np.random.randint( low = 500 , high = 5000 )

        logger.info('Resetting env')
        # self.state = np.zeros(shape=(1,5,150))
        logger.debug('self.state:{}'.format(self.state))
        self.state = None
        self.state = np.copy(self.X_train[self.batch_id].reshape(1, self.nsamples, self.nvariables))
        logger.debug('self.state:{}'.format(self.state))
        logger.debug('reset_data.shape:{}'.format(self.state.shape))
        self.min_BIMIN = unscale( self.variables[0] , self.state[: , : , 0] , self.scalers ).min( )
        self.max_BIMIN = unscale( self.variables[0] , self.state[: , : , 0] , self.scalers ).max( )
        logger.info('Lower and upper B:VIMIN: [{},{}]'.format(self.min_BIMIN, self.max_BIMIN))
        self.VIMIN = self.state[0 , 0 , -1 :]
        logger.debug('Normed VIMIN:{}'.format(self.VIMIN))
        # logger.debug('B:VIMIN:{}'.format(self.scalers[0].inverse_transform(np.array([self.VIMIN]).reshape(1, -1))))
        logger.debug( 'B:VIMIN:{}'.format(
            unscale( self.variables[0] , np.array( [self.VIMIN] ) , self.scalers ).reshape( 1 ,
                                                                                               -1 ) ) )
        return self.state[0, -1:, :].flatten()

    def render(self):

        plt.rcParams['axes.titlesize'] = 18
        plt.rcParams['axes.titleweight'] = 'bold'
        plt.rcParams['axes.labelsize'] = 18
        plt.rcParams['axes.labelweight'] = 'regular'
        plt.rcParams['xtick.labelsize'] = 14
        plt.rcParams['ytick.labelsize'] = 14
        plt.rcParams['font.family'] = [u'serif']
        plt.rcParams['font.size'] = 14

        logger.debug('render()')
        sns.set_style("ticks")
        nvars = 2
        fig, axs = plt.subplots(nvars, figsize=(12, 8))
        logger.debug('self.state:{}'.format(self.state))
        for v in range(0, nvars):
            utrace = self.state[0, :, v]
            trace = unscale( self.variables[v] , utrace.reshape( -1 , 1 ) ,
                                  self.scalers )
            if v == 0:
                iminer_imp = 0
                if self.total_iminer > 0:
                    iminer_imp = self.data_total_iminer / self.total_iminer
                axs[v].set_title('Raw data reward: {:.4f} - RL agent reward: {:.4f} - Improvement: {:.2f} '.format(self.data_total_reward,
                                                                                                                   self.total_reward, iminer_imp))
            axs[v].plot(trace, label='Digital twin', color='black')

            # if v==1:
            data_utrace = self.data_state[0, :, v]
            data_trace =unscale( self.variables[v] , data_utrace.reshape( -1 , 1 ) ,
                                  self.scalers )
            # data_trace = self.scalers[v].inverse_transform(data_utrace.reshape(-1, 1))
            if v == 1:
                x = np.linspace(0, LOOK_BACK-1 , LOOK_BACK)
                axs[v].fill_between(x, -data_trace.flatten(), +data_trace.flatten(), alpha=0.2, color='red')
            if v == 0:
                axs[v].set_ylim([103.2,103.6])
            axs[v].plot(data_trace, 'r--', label='Data')
            axs[v].set_xlabel('time')
            axs[v].set_ylabel('{}'.format(self.variables[v]))
            axs[v].legend(loc='upper left')
        plt.savefig ( EPISODES_PLOTS_DIR + '/' + 'arch_{}_dqn_int{}_out{}_env_EXB_{}_train_episode_{}_step_{}.png'.format ( ARCH_TYPE, len(VARIABLES), OUTPUTS,ENV_VERSION,self.episodes , self.steps ) )
        plt.close('all')
